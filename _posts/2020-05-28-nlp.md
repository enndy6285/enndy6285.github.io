---
layout: post
title: Text Preprocessing for NLP
---

### Introduction
Yelp is one of the largest platforms for customer generated business reviews with over 211 milion cumulative reviews to date. This wealth of information is valuable not only to consumers but the businesses themselves. A five star rating is the goal of every restaurant but what distinguishes a 5-star restaurant?

### Objective
The goal of this project is to use NLP techniques to analyze Yelp text reviews in order to gain insight on features that are predictive of a 5-star rating.

### The Data
Data was obtained from the Yelp Open Dataset available at [Yelp Open Dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset). It was uploaded into MongoDB. A subset consisting of Toronto restaurant reviews was then downloaded for analysis

### Methodology
Today we will focus on text preprocessing and how this affects our final analysis. Output we will be looking at will be topics generated after text vectorization (TF-IDF) and topic modeling (NMF, number of topics set at 50).

### Text Preprocessing
Text preprocessing serves as the backbone of NLP. The phrase "garbage in, garbage out" certainly applies here. 

#### Basic Text Cleaning
First we will do some basic cleaning including removing special characters, punctuation, and capitalizations. This can be performed with regular expressions:

>   text = re.sub('\w*\d\w*', ' ', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text.lower())

>Topic  0 - salad, steak, dressing, greek, pasta, ordered, kale, caesar, bread, side
>Topic  1 - food, service, excellent, fast, slow, bad, authentic, ok, indian, customer
>Topic  2 - us, table, came, server, asked, minutes, told, said, waitress, got
>Topic  3 - sushi, sashimi, roll, rolls, salmon, ayce, tempura, fish, japanese, maki
>Topic  4 - pizza, crust, toppings, pizzas, pasta, thin, slice, italian, sauce, pepperoni
>Topic  5 - ramen, broth, tonkotsu, noodles, miso, egg, kinton, bowl, spicy, shoyu



#### Stemming

#### Stop Words

#### N-grams





### Error Metrics in Multi-Class
Before diving deeper into the models, it was important to decide what metrics I would be evaluating them on. In binary classification problems, standard metrics may include accuracy, precision, recall, and f1. For multi-class, we can calculate these same metrics by converting the data into a series of binary problems, ie One-vs-all. This will give us 3 values for each metric, one for each outcome class. Additionally, we can calculate an overall score by implementing a weighting scheme. There are 2 primary weighting options - micro vs macro. In micro weighting, each data point is given equal weight which will naturally give more weight to majority classes. In macro weighting, metrics are calculated for each class individually and then averaged together. This can give more weight to minority classes relative to micro weighting.

*Note: In micro-weighted scores, Accuracy = Precision = Recall = F1. 

Which score is best depends on what goals we have. Thinking back to the case problem, I decided on the following priorites:
#### 1. Maximize recall of going undrafted
The worst case scenario is a player expecting to be drafted (potentially giving up college eligibility) and then failing to do so. To minimize this I want to reduce the number of false negatives when predicting going undrafted. This would be reflected in higher recall scores for class 3. 

#### 2. Maximize both precision and recall of predicting going in the 1st round
In predicting which players will go in the 1st round, I care about precision and recall equally. Similar to above, I want to minimize players potentially giving up college eligibility expecting to go in the first round, only to drop to the 2nd round or lower. At the same time, there is so much to be gained from being drafted in the 1st round, that I want to capture as much of those cases as possible. F1 score of class 1 would give a good balance of both recall and precision.

From these 2 priorities, it's implied that predicting class 3 carries the most weight, followed by class 1, and then class 2. This happens to align with the natural imbalance in the dataset. Therefore, this can be used to our advantage with micro-weighted averages when looking at an overall accuracy score.

In summary, I will use these metrics to evaluate the classification models:
1. Class 0 Recall
2. Class 1 F1
3. Micro-weighted Accuracy

### Selecting a Model
The data was split into training and test sets. After running the 3 classification models on the training data with 10-fold cross validation, logistic regression and random forest appeared to perform best. The error metrics for both models are as follows:

Logistic Regression metrics:
![Logistic Regression scores]({{ site.url }}/images/2020-05-12/lr_val_table.png)

Random Forest metrics:
![Random Forest scores]({{ site.url }}/images/2020-05-12/rf_val_table.png)

Although the random forest model has slightly higher overall accuracy, it performs lower in the first two metrics of interest - class 0 recall and class 1 F1. Therefore, I decided to use logistic regression as my final model. After retraining on the entire training set, here is how it performs on the set-aside test data:

![Logistic Regression Confusion Matrix]({{ site.url }}/images/2020-05-12/lr_test_matrix.png)

The final result is a classification model that prioritizes class 1 and class 3. In fact, it weights these 2 classes to the point that the model resembles binary classification with almost no predictions made for class 2. With more time, it would be interesting to see how the models would look with additional fine-tuning of class weights.

### Conclusion
In order to properly evaluate a model, it is necessary to determine what metrics will be used for evaluation. This is especially true for multi-class problems which can be more difficult to summarize with a single measure. 